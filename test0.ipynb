{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, TextVectorization\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/train.csv\", encoding = \"ISO-8859-1\", names=['polarity', 'id', 'query', 'user', 'text'], index_col=2)\n",
    "df = df.sample(frac=1)[:60_000] # shuffle and truncate\n",
    "df['polarity'] = df['polarity'].apply(lambda x: 1 if x == 4 else 0)\n",
    "\n",
    "tdf, vdf = train_test_split(df, test_size=0.2)\n",
    "train_data = tdf['text'].to_numpy()\n",
    "train_label = tdf['polarity'].to_numpy()\n",
    "\n",
    "val_data = vdf['text'].to_numpy()\n",
    "val_label = vdf['polarity'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 200_000  # full vocab for 1.6M dataset contains 850061 tokens\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64041\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'i', 'to', 'the', 'a', 'my', 'and', 'you', 'is', 'it',\n",
       "       'in', 'for', 'of', 'im', 'on', 'me', 'so', 'have', 'that', 'but',\n",
       "       'just', 'with', 'be', 'at', 'its', 'not', 'was', 'this', 'now',\n",
       "       'good', 'up', 'day', 'out', 'get', 'all', 'are', 'like', 'go',\n",
       "       'no'], dtype='<U85')"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = np.array(encoder.get_vocabulary())\n",
    "word_index = dict(zip(vocab, range(len(vocab))))\n",
    "\n",
    "print(len(vocab))\n",
    "vocab[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int64, numpy=array([459, 884,   4, 274,  43])>"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(\"Hello, how's the weather today?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = \"glove/glove.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 28022 words (36019 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(vocab) + 2\n",
    "embedding_dim = 200\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "\n",
    "embedding_layer = Embedding(\n",
    "    num_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding_layer(encoder('I am'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    embedding_layer,\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([layer.supports_masking for layer in model.layers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "[-0.30668777]\n",
      "1/1 [==============================] - 0s 240ms/step\n",
      "[-0.30668783]\n"
     ]
    }
   ],
   "source": [
    "sample_text = ('the gig last night turned out great')\n",
    "predictions = model.predict(np.array([sample_text]))\n",
    "print(predictions[0])\n",
    "\n",
    "padding = \"the \" * 2000\n",
    "predictions = model.predict(np.array([sample_text, padding]))\n",
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 63s 37ms/step - loss: 0.5319 - accuracy: 0.7007\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 62s 41ms/step - loss: 0.4697 - accuracy: 0.7614\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 57s 38ms/step - loss: 0.4551 - accuracy: 0.7702\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 61s 40ms/step - loss: 0.4459 - accuracy: 0.7785\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 63s 42ms/step - loss: 0.4375 - accuracy: 0.7834\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 58s 39ms/step - loss: 0.4289 - accuracy: 0.7891\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 56s 37ms/step - loss: 0.4214 - accuracy: 0.7933\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 58s 39ms/step - loss: 0.4135 - accuracy: 0.7985\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 61s 41ms/step - loss: 0.4059 - accuracy: 0.8029\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 59s 39ms/step - loss: 0.3979 - accuracy: 0.8077\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(x=train_data, y=train_label, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.9692551 ],\n",
       "       [-2.8697622 ],\n",
       "       [-0.9298316 ],\n",
       "       [-4.233221  ],\n",
       "       [ 3.431041  ],\n",
       "       [ 2.0594976 ],\n",
       "       [ 2.9084666 ],\n",
       "       [ 1.3246045 ],\n",
       "       [ 0.22528376],\n",
       "       [ 2.2659986 ]], dtype=float32)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(np.array([\"Shit!\", \"Oh fuck!\", \"You'd better shut up!\", \"I wanna kill this bastard\", \"That was amazing\", \"OMG!\"\n",
    "                        \"That was cute\", \"Nice one\", \"I loved it\", \"I really liked how he behaved\", \"He was a nice dude\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
